{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus: **The Critique of Pure Reason** by Immanuel Kant\n",
    "http://www.gutenberg.org/files/4280/4280.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(\"corpera/kant.txt\", 'r') as f: \n",
    "    for line in f:\n",
    "            line = line.strip().lower()\n",
    "            line = line.decode(\"ascii\", \"ignore\")\n",
    "            if (len(line)==0):\n",
    "                continue\n",
    "            lines.append(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = lines[17:]  # skip the first few lines of the file which are not part of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \" \".join(lines) # turns the book into a stream of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn text into stream of characters and create lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discard_chars = set([c for c in '01234567890%=@$&+*[]<>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = [c for c in text if c not in discard_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = set(text)\n",
    "nb_chars = len(chars)\n",
    "char2index = dict((c,i) for i,c in enumerate(chars))\n",
    "index2char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct input/output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQLEN = 20\n",
    "STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seqs = []\n",
    "target_chars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_seqs.append(text[i: i+SEQLEN])\n",
    "    target_chars.append(text[i+SEQLEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(input_seqs), len(target_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seqs[300000], target_chars[300000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_seqs), SEQLEN, nb_chars), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.zeros((len(input_seqs), nb_chars), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An individual input will be a one-hot encoded sequence of 20 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, input_char in enumerate(input_seqs):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "        Y[i, char2index[target_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model = Sequential()\n",
    "rnn_model.add(SimpleRNN(units=100, return_sequences=False, unroll=True,\n",
    "                   input_shape=(SEQLEN, nb_chars)))\n",
    "rnn_model.add(Dense(nb_chars))\n",
    "rnn_model.add(Activation(\"softmax\"))\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: replace the SimpleRNN with an LSTM or GRU.  Any difference in model quality or training times?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training/testing procedure\n",
    "  * Train once through the whole dataset\n",
    "  * Generate some text and judge quality\n",
    "  * Repeat until quality is satisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    print\n",
    "    print \"----------- Iteration number {} --------------\".format(i)\n",
    "    # train\n",
    "    history = rnn_model.fit(X,Y, batch_size=128, epochs=1, validation_split=0.1)\n",
    "    # generate.\n",
    "    # get a random input:\n",
    "    test_idx = np.random.randint(len(input_seqs))\n",
    "    test_chars = input_seqs[test_idx]\n",
    "    print \"generating from: {}/\".format(''.join(test_chars)),\n",
    "    for i in range(100): # characters to generate after seed\n",
    "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for i, ch in enumerate(test_chars):\n",
    "            Xtest[0, i, char2index[ch]] = 1\n",
    "        pred = rnn_model.predict(Xtest, verbose=0)[0]  # returns the whole softmaxed array\n",
    "        ypred = index2char[np.argmax(pred)]  # take the highest probability\n",
    "        print ypred,\n",
    "        test_chars = test_chars[1:] + [ypred]  # construct next input sequence\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Try and fix the repetition problem.  Instead of choosing the character with maximal probability after each call to `predict`, sample from the distribution that `predict` returns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network which takes a sequence as input and outputs a sequence (which may be of variable length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * We'll build our own Seq2Seq with base Keras (derived from example)\n",
    "  * We'll look at some 3rd party layers built for Keras\n",
    "  * Lastly we'll try some command-line implementations of seq2seq (Google and Neural Monkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original seq2seq paper: http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras example: addition as a text translation problem\n",
    "\n",
    "input: 123+42 (a sequence of 0-9 and +)\n",
    "\n",
    "output (hopefully): 165 (a sequence of 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 100000\n",
    "DIGITS = 3\n",
    "INVERT = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []  # will hold the addition problems\n",
    "expected = []  # will hold the answer to each addition problems\n",
    "seen = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 100000\n"
     ]
    }
   ],
   "source": [
    "print 'Generating data...'\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if INVERT:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print 'Total addition questions:', len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   41+8'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the questions (one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 7, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  # samples, input sequence length, vocabulary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False],\n",
       "       [False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False],\n",
       "       [False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False]], dtype=bool)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 4, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False, False, False,\n",
       "        False, False,  True],\n",
       "       [False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 4, 12)             1548      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s2s_model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "\n",
    "s2s_model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "s2s_model.add(layers.RepeatVector(DIGITS + 1))\n",
    "\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    s2s_model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "s2s_model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "s2s_model.add(layers.Activation('softmax'))\n",
    "s2s_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "s2s_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 37s - loss: 1.3875 - acc: 0.4853 - val_loss: 1.2516 - val_acc: 0.5427\n",
      "Q 91+453 \n",
      "T 544 \n",
      "\u001b[91m☒\u001b[0m  525 \n",
      "---\n",
      "Q 52+938 \n",
      "T 990 \n",
      "\u001b[91m☒\u001b[0m  900 \n",
      "---\n",
      "Q 31+74  \n",
      "T 105 \n",
      "\u001b[91m☒\u001b[0m  117 \n",
      "---\n",
      "Q 87+844 \n",
      "T 931 \n",
      "\u001b[91m☒\u001b[0m  921 \n",
      "---\n",
      "Q 372+74 \n",
      "T 446 \n",
      "\u001b[91m☒\u001b[0m  438 \n",
      "---\n",
      "Q 494+35 \n",
      "T 529 \n",
      "\u001b[91m☒\u001b[0m  421 \n",
      "---\n",
      "Q 233+40 \n",
      "T 273 \n",
      "\u001b[91m☒\u001b[0m  353 \n",
      "---\n",
      "Q 386+23 \n",
      "T 409 \n",
      "\u001b[91m☒\u001b[0m  400 \n",
      "---\n",
      "Q 584+930\n",
      "T 1514\n",
      "\u001b[91m☒\u001b[0m  1503\n",
      "---\n",
      "Q 645+94 \n",
      "T 739 \n",
      "\u001b[91m☒\u001b[0m  730 \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 2): ## increase from 2 --> 200 to get a fully-trained model\n",
    "    print\n",
    "    print '-' * 50\n",
    "    print 'Iteration', iteration\n",
    "    s2s_model.fit(x, y,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_split=0.1)\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x))\n",
    "        rowx, rowy = x[np.array([ind])], y[np.array([ind])]\n",
    "        preds = s2s_model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print 'Q', q[::-1] if INVERT else q\n",
    "        print 'T', correct \n",
    "        if correct == guess:\n",
    "            print colors.ok + '☑' + colors.close + \" \",\n",
    "        else:\n",
    "            print colors.fail + '☒' + colors.close + \" \",\n",
    "        print guess \n",
    "        print '---'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise: implement a convolutional seq2seq algorithm!  See: https://arxiv.org/abs/1705.03122 and https://github.com/facebookresearch/fairseq**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequential character auto-encoder from Keras addition example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have a large collection of short texts, Tweets, titles, etc., and you'd like to cluster them for purposes of characterization, classification, or anomaly detection.  If you can first represent each text as a dense vector, you can then apply clustering and visualization algorithms.  \n",
    "\n",
    "Auto-encoding (transforming the text into something else, then back into itself) is one way to create a dense vector that represents a bit of text.  At the end of the encoding process, place a dense layer of the desired dimension.  You'll be able to extract these dense vectors after training.\n",
    "\n",
    "It's not difficult to turn any seq2seq model into an auto-encoder.  Notice that a seq2seq model has an encoder stage followed by a decoder stage.  In the addition example, the encoder stage is simply the first RNN layer which output a vector of HIDDEN_SIZE (default 128).  Verify this by examining `s2s_model.summary()`.\n",
    "\n",
    "Therefore, to change the addition example into an auto-encoder, place a dense layer right after the first RNN layer.  This will take the 128 (default) dimensional vector down to whatever you wish (try 10). \n",
    "\n",
    "Since you've added an extra twist in the encoder, you'll have to undo-it to use the existing decoder.  This means you need to add _another_ dense layer to take the dimension back up to 128.  Finally, you'll need the length of the output sequence to be the same as the input sequence.  This is specified in the `RepeatVector` layer.  Here's how it all looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "ENCODED_VECTOR_DIM = 10\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rnn_encoder (LSTM)           (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "encoded_dense (Dense)        (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "upsample (Dense)             (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 7, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 7, 12)             1548      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 7, 12)             0         \n",
      "=================================================================\n",
      "Total params: 208,022\n",
      "Trainable params: 208,022\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s2s_model = Sequential()\n",
    "\n",
    "## ENCODER STAGER\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "s2s_model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars)), name=\"rnn_encoder\"))\n",
    "# add a dense layer to the encoding\n",
    "s2s_model.add(layers.Dense(ENCODED_VECTOR_DIM, name=\"encoded_dense\"))\n",
    "# upsample for the decoder\n",
    "s2s_model.add(layers.Dense(HIDDEN_SIZE, name=\"upsample\"))\n",
    "# adding a dropout layer really helps both training speed and the ultimate accuracy of the model\n",
    "s2s_model.add(layers.Dropout(DROPOUT))\n",
    "\n",
    "## DECODER STAGE\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. \n",
    "s2s_model.add(layers.RepeatVector(MAXLEN))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    s2s_model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "s2s_model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "s2s_model.add(layers.Activation('softmax'))\n",
    "s2s_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "s2s_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, pass it the input again as the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "95000/95000 [==============================] - 49s - loss: 0.2376 - acc: 0.9156 - val_loss: 0.1627 - val_acc: 0.9470\n",
      "Epoch 2/2\n",
      "95000/95000 [==============================] - 49s - loss: 0.1900 - acc: 0.9322 - val_loss: 0.1319 - val_acc: 0.9556\n"
     ]
    }
   ],
   "source": [
    "history = s2s_model.fit(x, x, epochs=2, batch_size=128, shuffle=True, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you extract the encoded dense-vectors?  Keras can make a new model for you, starting with your current input, and ending with the encoded vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=s2s_model.get_layer('rnn_encoder').input, outputs=s2s_model.get_layer('encoded_dense').output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the encoder, simply pass our input data to its predict method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded = encoder_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93454826, -0.71101081,  1.64225972, -0.73856395,  0.79330266,\n",
       "        1.74125302,  0.53821278, -1.94646728, -0.07050987, -2.60903144], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!  You've turned an addition problem into a 10-d vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: run a dimensionality reduction technique, such as PCA or t-SNE, on the collection of 10-d vectors, to reduce them to 2-d vectors.  Plot them.  Did you achieve a good clustering of addtition problems?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: extract the decoder.  Since you are not starting with an existing input, Keras requires you to work a little harder.  Using the functional API, set up an input for the 10-d encoded vectors, and add the decoder layers one-by-one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd party Keras layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * https://github.com/farizrahman4u/recurrentshop\n",
    "  * https://github.com/farizrahman4u/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `recurrentshop` by cloning the repo then running the install script (`python setup.py install`). \n",
    "\n",
    "Install `seq2seq` with `pip install git+https://github.com/farizrahman4u/seq2seq.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seq2seq.models import SimpleSeq2Seq\n",
    "\n",
    "s2s_model = BasicSeq2Seq(input_dim=SEQLEN, hidden_dim=10, output_length=1, output_dim=nb_chars)\n",
    "s2s_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BasicSeq2Seq?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standard2",
   "language": "python",
   "name": "standard2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
